# 필요한 라이브러리 import
!pip install konlpy #한국어 NLP
from google.colab import drive 
drive.mount('/content/gdrive/')

import pandas as pd
import urllib.request
%matplotlib inline
import matplotlib.pyplot as plt
import re
from konlpy.tag import Okt # 한국어 자연어 처리기(형태소 분석기)_어느 정도의 띄어쓰기 되어 있는 "인터넷" 영화평/상품명을 처리
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os
from tensorflow.keras.datasets import reuters
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical # 원핫인코딩으로 변환
from tensorflow.keras.models import load_model
from tqdm import tqdm # Progressbar 생성

# 데이터 불러오기
train = pd.read_csv('/content/gdrive/My Drive/train.csv', encoding='utf-8')
test = pd.read_csv('/content/gdrive/My Drive/test.csv', encoding='utf-8')
sample_submission = pd.read_csv('/content/gdrive/My Drive/sample_submission.csv', encoding = 'utf-8')

# 데이터 전처리
train = train.dropna(how = 'any')
train['data'] = train['data'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
test['data'] = test['data'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','을']
# 형태소 분석기 okt
okt = Okt()

# train 데이터 전처리(형태소 추출 및 불용어 제외)
X_train = []
for sentence,i in zip(train['data'],tqdm(range(len(train['data'])))) :
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True)
    temp_X = [word for word in temp_X if not word in stopwords]
    X_train.append(temp_X)

# test 데이터 전처리
X_test = []
for sentence in test['data']:
    temp_X = []
    temp_X = okt.morphs(sentence, stem=True)
    temp_X = [word for word in temp_X if not word in stopwords]
    X_test.append(temp_X)
    
# 인스턴스화
tokenizer = Tokenizer()

# 단어사전 만들기 
tokenizer.fit_on_texts(X_train)

vocab_size = 30000
tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

max_len = 500
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)

y_train = to_categorical(train['category'])

# 모델 생성 및 훈련
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import Huber
from tensorflow.keras.layers import Dropout, Bidirectional
from tensorflow.keras.callbacks import ModelCheckpoint
model = Sequential([
    Embedding(vocab_size, 120),
    Bidirectional(LSTM(120,return_sequences = True)),
    Bidirectional(LSTM(120)),
    Dropout(0.5),
    Dense(30),
    Dense(3, activation = 'softmax')
])
model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])

checkpoint_path = 'my_checkpoint.ckpt'
checkpoint = ModelCheckpoint(filepath = checkpoint_path,
                             save_best_only = True,
                             save_weights_only = True,
                             monitor = 'acc')

history = model.fit(X_train, y_train,
                    batch_size = 128, epochs = 15, verbose = 1,
                    callbacks = [checkpoint])
model.load_weights(checkpoint_path)

# 예측 및, submission 파일 생성
y_pred = model.predict_classes(X_test)
sample_submission['category'] = y_pred
sample_submission.to_csv('/content/gdrive/My Drive/submission2.csv', encoding='utf-8', index = False)

from tensorflow import keras
from tensorflow.keras import layers
# 모델 저장 및 로드
# 모델 저장 
model.save(r"c:\tfproject\Bluehouse_model1.h5")  # 'c:/temp/mymodel.h5'



# 모델 제거
#del model  



# 모델 읽어오기
#model = keras.models.load_model(r'c:\temp\mymodel.h5') # 'c:/temp/mymodel.h5'



# 모델 정보 출력
model.summary()
