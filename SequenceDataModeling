# 필요한 모듈 import
import tensorflow as tf
import numpy as np
import urllib
import csv

from tensorflow.keras.layers import Dense, Conv1D, LSTM, Lambda
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import Huber

# CSV 파일로부터 데이터셋 만들기
url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'
urllib.request.urlretrieve(url, 'sunspots.csv')

# csv 파일 읽어오기 (임시출력후, 데이터셋 만들기)
with open('sunspots.csv') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader) # 첫줄 건너뛰기(표 머릿말)
    i = 0
    for row in reader:
        print(row)
        i+=1
        if i > 10:
            break

with open('sunspots.csv') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    next(reader)
    i = 0
    for row in reader:
        print(row)
        i+=1
        if i > 10:
            break

sunspots = []
time_step = []

with open('sunspots.csv') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    # 첫 줄은 header이므로 skip 합니다.
    next(reader)
    for row in reader:
        sunspots.append(float(row[2]))
        time_step.append(int(row[0]))
# 넘파이 array로 변경
series = np.array(sunspots)
time = np.array(time_step)

# 태양의 흑점 활동 시각화
import matplotlib.pyplot as plt

plt.figure(figsize=(16, 8))
plt.plot(time, series)
plt.xlabel("Time")
plt.ylabel("Value")
plt.grid(True)

# Train set, validation Set 생성
split_time = 3000
time_train = time[:split_time]
time_valid = time[split_time:]

x_train = series[:split_time]
x_valid = series[split_time:]

# Window Dataset Loader

window_size = 30
batch_size = 32
shuffle_size = 1000

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

# train_set / validation_set 만들기
train_set = windowed_dataset(x_train, 
                             window_size=window_size, 
                             batch_size=batch_size,
                             shuffle_buffer=shuffle_size)

validation_set = windowed_dataset(x_valid, 
                                  window_size=window_size,
                                  batch_size=batch_size,
                                  shuffle_buffer=shuffle_size)
 
# 모델 정의
model = Sequential([
                    Conv1D(60, kernel_size = 5, padding = 'causal', activation = 'relu', input_shape = [None, 1]),
                    LSTM(60, return_sequences = True),
                    LSTM(60, return_sequences = True),
                    Dense(30, activation = 'relu'),
                    Dense(10, activation = 'relu'),
                    Dense(1),
                    Lambda(lambda x: x * 400)
])

# compile
optimizer = SGD(lr = 1e-5, momentum = 0.9)
loss = Huber()
model.compile(loss = loss, optimizer = optimizer, metrics = ['mae'])

# ModelCheckpoint
checkpoint_path = 'my_checkpoint.ckpt'
checkpoint = ModelCheckpoint(checkpoint_path, 
                             save_weights_only=True, 
                             save_best_only=True, 
                             monitor='val_mae',
                             verbose=1)
                             
model.load_weights(checkpoint_path)

# 학습 오차에 대한 시각화
plt.figure(figsize=(12, 9))
plt.plot(np.arange(1, epochs+1), history.history['loss'])
plt.plot(np.arange(1, epochs+1), history.history['val_loss'])
plt.title('Loss / Val Loss', fontsize=20)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['loss', 'val_loss'], fontsize=15)
plt.show()

plt.figure(figsize=(12, 9))
plt.plot(np.arange(1, epochs+1), history.history['mae'])
plt.plot(np.arange(1, epochs+1), history.history['val_mae'])
plt.title('MAE / Val MAE', fontsize=20)
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend(['mae', 'val_mae'], fontsize=15)
plt.show()
